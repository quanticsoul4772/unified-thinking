# Comprehensive Test Plan - Unified Thinking MCP Server
## Tool Validation & Accuracy Assessment

**Generated by Expert Agent Analysis**
**Date**: 2025-10-03

---

## Executive Summary

This test plan provides a systematic approach to validate the accuracy and effectiveness of all 39 tools in the unified-thinking MCP server. Based on multi-agent analysis, we've identified **critical gaps**, **accuracy benchmarks**, and **improvement priorities**.

### Key Findings

**Current State**:
- **Overall Test Coverage**: 73.6%
- **Critical Gaps**: Argument Analysis (0%), Orchestration (0%), Enhanced Tools (0%)
- **Fundamental Issues**: Pattern matching instead of formal logic, broken Bayesian math, no real causal inference

**Critical Weaknesses Identified**:
1. ⚠️ **String matching masquerading as logic** - 90%+ error miss rate
2. ⚠️ **Broken Bayesian updates** - Violates probability theory
3. ⚠️ **No formal proof system** - Pattern matching only
4. ⚠️ **Keyword-based fallacy detection** - High false positive rate
5. ⚠️ **No causal inference theory** - No Pearl's do-calculus

---

## 1. Tool Inventory & Categorization

### 1.1 Core Tools (31 Original + 8 Enhanced = 39 Total)

#### **Category A: Critical Path Tools** (Priority 0)
Tools that other tools depend on or that perform foundational reasoning:

1. **think** - Main thinking engine
2. **validate** - Logical consistency (⚠️ CRITICAL FLAW: string matching only)
3. **prove** - Logical proofs (⚠️ CRITICAL FLAW: no real inference)
4. **probabilistic-reasoning** - Bayesian updates (⚠️ CRITICAL FLAW: wrong math)
5. **build-causal-graph** - Causal modeling (⚠️ CRITICAL: no real causal theory)

#### **Category B: Analysis Tools** (Priority 1)
6. **detect-contradictions**
7. **detect-biases** (⚠️ Only 6/180+ biases)
8. **self-evaluate**
9. **detect-fallacies** (⚠️ NEW - keyword matching)
10. **decompose-argument** (⚠️ NEW - 0% tested)
11. **generate-counter-arguments** (⚠️ NEW - 0% tested)

#### **Category C: Decision & Planning** (Priority 1)
12. **make-decision**
13. **decompose-problem**
14. **sensitivity-analysis**
15. **analyze-perspectives**
16. **analyze-temporal**
17. **identify-optimal-timing**

#### **Category D: Integration Tools** (Priority 2)
18. **synthesize-insights**
19. **detect-emergent-patterns**
20. **execute-workflow** (⚠️ NEW - 0% tested)
21. **process-evidence-pipeline** (⚠️ NEW - 0% tested)
22. **analyze-temporal-causal-effects** (⚠️ NEW - 0% tested)

#### **Category E: Analogical & Creative** (Priority 2)
23. **find-analogy** (⚠️ NEW - minimal testing)
24. **apply-analogy** (⚠️ NEW - minimal testing)

#### **Category F: Causal Reasoning** (Priority 1)
25. **simulate-intervention**
26. **generate-counterfactual**
27. **analyze-correlation-vs-causation**
28. **get-causal-graph**

#### **Category G: Infrastructure** (Priority 3)
29. **history**
30. **search**
31. **list-branches**
32. **focus-branch**
33. **branch-history**
34. **get-metrics**
35. **recent-branches**
36. **assess-evidence**
37. **check-syntax**
38. **compare-time-horizons**
39. **list-workflows** (⚠️ NEW)

---

## 2. Accuracy Validation Approach

### 2.1 "Good" Performance Criteria

#### **Logical Reasoning Tools**
- **Soundness**: Valid inferences must be correct 100% of the time
- **Completeness**: Should find proofs when they exist ≥90% of the time
- **Accuracy**: Truth value determination ≥95%
- **Current Reality**: ⚠️ ~10% accuracy (string matching)

#### **Probabilistic Reasoning Tools**
- **Calibration**: Expected Calibration Error (ECE) <0.05
- **Brier Score**: <0.2 (lower is better)
- **Bayesian Accuracy**: MAE from correct posterior <0.01
- **Current Reality**: ⚠️ Mathematically incorrect formula

#### **Causal Reasoning Tools**
- **Structure F1**: Graph recovery F1 score >0.8
- **Intervention MAE**: Mean absolute error <0.15
- **Confounder Detection**: Precision >85%, Recall >80%
- **Current Reality**: ⚠️ No formal causal theory implemented

#### **Fallacy Detection**
- **Precision**: >88% (low false positives)
- **Recall**: >82% (catch most fallacies)
- **F1 Score**: ≥0.77 per fallacy type
- **Current Reality**: ⚠️ Keyword matching = high false positive rate

#### **Argument Analysis**
- **Premise Extraction Recall**: ≥90%
- **Hidden Assumption Detection**: ≥75%
- **Argument Type Classification**: ≥85%
- **Current Reality**: ⚠️ 0% test coverage

### 2.2 Benchmarks & Baselines

#### **Standard Datasets**
1. **FOLIO** (First-Order Logic) - 1,435 examples
2. **LogiQA** - 8,678 logical reasoning questions
3. **CLEVRER** - Causal reasoning video dataset
4. **CausalBank** - 3,000+ causal relationship annotations
5. **Fallacy Corpus** - Labeled fallacy dataset (20+ types)

#### **Baseline Systems**
- **Prolog/Z3** for logical reasoning (target: 90% of their accuracy)
- **PyMC3** for Bayesian inference (target: MAE within 0.01)
- **DoWhy** for causal inference (target: 85% F1 score)
- **Human experts** for argument analysis (target: 75% agreement)

---

## 3. Critical Test Scenarios

### 3.1 Logical Reasoning Tests (CRITICAL - Fix First)

#### **Test Suite 1: Formal Logic Validation**
```
Total Tests: 100
Success Criteria: ≥95% accuracy

Current Implementation Issues:
- No propositional logic parser
- No first-order logic support
- String matching instead of semantic evaluation
```

**Test Cases**:

1. **Modus Ponens** (20 cases)
   ```
   Premise 1: If P then Q
   Premise 2: P
   Conclusion: Q
   Expected: VALID
   Current: ~50% accuracy (keyword matching)
   ```

2. **Modus Tollens** (20 cases)
   ```
   Premise 1: If P then Q
   Premise 2: Not Q
   Conclusion: Not P
   Expected: VALID
   Current: ~40% accuracy
   ```

3. **Universal Instantiation** (15 cases)
   ```
   Premise: All humans are mortal
   Instance: Socrates is human
   Conclusion: Socrates is mortal
   Expected: VALID
   Current: ~60% accuracy (pattern matching)
   ```

4. **Existential Generalization** (15 cases)
   ```
   Premise: Socrates is mortal
   Conclusion: Something is mortal
   Expected: VALID
   Current: NOT IMPLEMENTED
   ```

5. **Invalid Inferences** (30 cases)
   ```
   Test: Affirming the Consequent
   Premise 1: If P then Q
   Premise 2: Q
   Conclusion: P
   Expected: INVALID
   Current: Often marked VALID (false positive)
   ```

#### **Test Suite 2: Contradiction Detection**
```
Total Tests: 50
Success Criteria: ≥90% precision, ≥85% recall

Current Issues:
- "The cat is black and not white" flagged as contradiction
- Misses semantic contradictions
- No modal logic support
```

**Test Cases**:

1. **Direct Contradictions** (15 cases)
   ```
   Statement: "X is true and X is false"
   Expected: CONTRADICTION
   Current: 70% accuracy
   ```

2. **Semantic Contradictions** (20 cases)
   ```
   Statement: "The bachelor is married"
   Expected: CONTRADICTION
   Current: MISSED (no semantic understanding)
   ```

3. **Non-Contradictions** (15 cases)
   ```
   Statement: "The cat is black and the dog is not white"
   Expected: NO CONTRADICTION
   Current: FALSE POSITIVE (string matching "and not")
   ```

### 3.2 Probabilistic Reasoning Tests (CRITICAL - Fix Math)

#### **Test Suite 3: Bayesian Inference**
```
Total Tests: 50
Success Criteria: MAE <0.01 from correct posterior

Current Issues:
- Formula violates probability theory
- No P(E|¬H) term in denominator
- Base rate often arbitrary (0.5)
```

**Test Cases**:

1. **Classic Bayes Examples** (20 cases)
   ```
   Test: Medical Testing
   P(Disease) = 0.01
   P(Positive|Disease) = 0.99
   P(Positive|¬Disease) = 0.05

   Correct: P(Disease|Positive) = 0.166
   Current: WRONG (missing complementary probability)
   ```

2. **Monty Hall Problem** (5 cases)
   ```
   Should switch doors
   P(Win|Switch) = 2/3
   P(Win|Stay) = 1/3

   Expected: Recommend SWITCH
   Current: LIKELY WRONG
   ```

3. **Evidence Combination** (25 cases)
   ```
   Test: Multiple independent evidence pieces
   Expected: Proper probability multiplication with normalization
   Current: Assumes independence (often wrong)
   ```

#### **Test Suite 4: Confidence Calibration**
```
Total Tests: 100
Success Criteria: ECE <0.05

Test: Predictions with confidence scores
Measure: How often 80% confident predictions are correct
Expected: ~80% should be correct
Current: NOT MEASURED
```

### 3.3 Causal Reasoning Tests (HIGH PRIORITY)

#### **Test Suite 5: Causal Graph Construction**
```
Total Tests: 75
Success Criteria: F1 >0.8 for edge recovery

Current Issues:
- No Pearl's do-calculus
- No confounding detection
- Keyword-based edge extraction
```

**Test Cases**:

1. **Simple Causal Chains** (25 cases)
   ```
   Description: "Smoking causes cancer, which causes death"
   Expected Graph: Smoking → Cancer → Death
   Current: ~60% accuracy (keyword matching)
   ```

2. **Confounding** (25 cases)
   ```
   Test: Ice cream sales and drowning deaths (summer confounds)
   Expected: Identify confounder, mark correlation not causation
   Current: LIKELY FAILS (no confounding detection)
   ```

3. **Collider Bias** (25 cases)
   ```
   Test: X → Z ← Y (Z is collider)
   Conditioning on Z creates spurious association X-Y
   Expected: Detect collider, warn about bias
   Current: NOT IMPLEMENTED
   ```

#### **Test Suite 6: Interventions**
```
Total Tests: 50
Success Criteria: MAE <0.15 from ground truth

Test: Predict do(X=x) effects
Measure: Compare to simulation or known causal model
Current: No do-calculus implementation
```

### 3.4 Fallacy Detection Tests (HIGH PRIORITY)

#### **Test Suite 7: Formal Fallacies**
```
Total Tests: 100 (20 fallacy types × 5 examples each)
Success Criteria: F1 ≥0.77 per type

Current Issues:
- Keyword matching only
- High false positive rate
- Misses context
```

**Test Cases**:

1. **Affirming the Consequent** (10 cases)
   ```
   Text: "If it rains, the ground is wet. The ground is wet. Therefore it rained."
   Expected: FALLACY DETECTED
   Current: ~50% accuracy
   ```

2. **False Dilemma** (10 cases)
   ```
   Text: "Either we ban all cars or accept pollution"
   Expected: FALSE DILEMMA
   Current: ~70% accuracy (looks for "either/or")
   ```

3. **Ad Hominem** (10 cases)
   ```
   Positive: "Your argument is wrong because you're an idiot"
   Expected: AD HOMINEM
   Current: 80% accuracy (keyword: "idiot")

   Negative: "The expert's methodology is flawed because..."
   Expected: NOT AD HOMINEM
   Current: FALSE POSITIVE (sees "expert")
   ```

#### **Test Suite 8: Statistical Fallacies**
```
Total Tests: 50
Success Criteria: F1 ≥0.75

Test: Base rate neglect, survivorship bias, Texas sharpshooter
Current: ~40% F1 (keyword matching)
```

### 3.5 Argument Analysis Tests (CRITICAL - 0% Coverage)

#### **Test Suite 9: Argument Decomposition**
```
Total Tests: 60
Success Criteria:
- Premise extraction recall ≥90%
- Hidden assumption detection ≥75%
- Argument type classification ≥85%

Current: 0% TEST COVERAGE, 500+ UNTESTED LINES
```

**Test Cases**:

1. **Premise Extraction** (20 cases)
   ```
   Argument: "We should reduce carbon emissions because climate change
             threatens humanity, and all threats to humanity require action."

   Expected Premises:
   - P1: Climate change threatens humanity
   - P2: All threats to humanity require action
   - P3: Reducing emissions addresses climate change (implicit)

   Hidden Assumptions:
   - Reducing emissions will effectively reduce climate change
   - Action on threats is feasible
   - The cost is justified by the benefit

   Current: UNTESTED
   ```

2. **Argument Type Classification** (20 cases)
   ```
   Test: Classify as deductive, inductive, or abductive

   Deductive: "All men are mortal. Socrates is a man. Therefore..."
   Expected: DEDUCTIVE

   Inductive: "The sun rose every day for 10,000 years. It will rise tomorrow."
   Expected: INDUCTIVE

   Abductive: "The grass is wet. Best explanation: it rained."
   Expected: ABDUCTIVE

   Current: UNTESTED
   ```

3. **Vulnerability Identification** (20 cases)
   ```
   Test: Identify weak premises, missing evidence, logical gaps
   Expected: List of specific vulnerabilities with severity
   Current: UNTESTED
   ```

#### **Test Suite 10: Counter-Argument Generation**
```
Total Tests: 30
Success Criteria: Generate relevant counter-arguments ≥75% of time

Test: Given an argument, generate counter-arguments using:
- Deny premise strategy
- Break inference link strategy
- Reductio ad absurdum strategy
- Alternative explanation strategy

Current: 0% TEST COVERAGE
```

### 3.6 Integration & Workflow Tests (CRITICAL - 0% Coverage)

#### **Test Suite 11: Workflow Orchestration**
```
Total Tests: 30
Success Criteria:
- Workflows execute correctly ≥95%
- Context preserved across steps ≥90%
- Dependencies respected 100%

Current: 0% TEST COVERAGE, 445+ UNTESTED LINES
```

**Test Cases**:

1. **Sequential Workflow** (10 cases)
   ```
   Workflow: comprehensive-analysis
   Steps: decompose → causal → temporal → perspectives → synthesis → decision

   Expected: All steps execute in order, context flows between steps
   Current: UNTESTED
   ```

2. **Parallel Workflow** (10 cases)
   ```
   Workflow: Multiple analyses in parallel
   Expected: All branches complete, results merged correctly
   Current: UNTESTED
   ```

3. **Conditional Workflow** (10 cases)
   ```
   Workflow: If confidence <0.7, run validation, else proceed
   Expected: Conditional execution based on intermediate results
   Current: UNTESTED
   ```

#### **Test Suite 12: Evidence Pipeline**
```
Total Tests: 25
Success Criteria:
- Evidence propagates to all linked systems
- Updates are mathematically correct
- No information loss

Current: 0% TEST COVERAGE
```

**Test Cases**:

1. **Evidence → Belief Updates** (10 cases)
   ```
   Test: New evidence updates Bayesian beliefs
   Expected: Correct Bayesian update (need to fix formula first!)
   Current: UNTESTED (and formula is wrong)
   ```

2. **Evidence → Causal Graph** (10 cases)
   ```
   Test: Evidence strengthens/weakens causal edges
   Expected: Edge confidence adjusted based on evidence quality
   Current: UNTESTED
   ```

3. **Evidence → Decision Updates** (5 cases)
   ```
   Test: Evidence changes decision recommendations
   Expected: Decision scores recalculated, recommendation updated
   Current: UNTESTED (feature stubbed out)
   ```

---

## 4. Self-Testing Strategy

### 4.1 Cross-Validation Approach

**Tools Test Each Other**:

1. **Logical + Probabilistic Cross-Check**
   ```
   Test: Probability assignments must satisfy logical constraints
   Example: If P(A) = 0.7 and P(B|A) = 0.8, then P(A∧B) ≤ min(0.7, 0.8)
   Implementation: Logical validator checks probabilistic outputs
   ```

2. **Causal + Temporal Consistency**
   ```
   Test: Causal effects must be temporally coherent
   Example: Cause must precede effect in time
   Implementation: Temporal reasoner validates causal graphs
   ```

3. **Argument Decomposition + Fallacy Detection**
   ```
   Test: Decomposed arguments checked for fallacies
   Implementation: Feed decomposition to fallacy detector
   ```

### 4.2 Property-Based Testing

**Invariant Properties**:

1. **Probability Coherence**
   - P(A) + P(¬A) = 1.0 (always)
   - P(A∧B) ≤ min(P(A), P(B))
   - P(A∨B) ≥ max(P(A), P(B))

2. **Logical Consistency**
   - If ⊢ P and ⊢ (P→Q), then ⊢ Q (modus ponens must work)
   - If ⊢ P and ⊢ ¬P, flag contradiction

3. **Causal Validity**
   - No cycles in causal DAGs
   - do(X=x) operations respect graph structure

### 4.3 Metamorphic Testing

**Transformations with Predictable Outcomes**:

1. **Logical Equivalence**
   ```
   Test: (P∧Q) should be equivalent to (Q∧P)
   Expected: Same validation result
   ```

2. **Probability Scaling**
   ```
   Test: Multiplying all evidence by constant should preserve rankings
   Expected: Belief ordering unchanged
   ```

3. **Argument Paraphrasing**
   ```
   Test: Logically equivalent argument formulations
   Expected: Same decomposition structure
   ```

---

## 5. Improvement Prioritization

### 5.1 Critical Fixes (Week 1-2)

**PRIORITY 0: Fix Fundamentally Broken Logic**

1. **Replace String Matching with Real Logic**
   - Implement propositional logic parser
   - Add first-order logic support
   - Use semantic evaluation, not keywords
   - **Effort**: 2 weeks, **Impact**: CRITICAL

2. **Fix Bayesian Math**
   - Correct formula: P(H|E) = P(E|H)P(H) / [P(E|H)P(H) + P(E|¬H)P(¬H)]
   - Add complementary probability handling
   - Implement proper normalization
   - **Effort**: 1 week, **Impact**: CRITICAL

3. **Add Real Causal Inference**
   - Implement Pearl's do-calculus
   - Add backdoor/frontdoor criteria
   - Implement confounding detection
   - **Effort**: 2 weeks, **Impact**: CRITICAL

### 5.2 High Priority (Week 3-5)

**PRIORITY 1: Test Untested Code**

4. **Argument Analysis Test Suite**
   - 60+ test cases for decomposition
   - 30+ test cases for counter-arguments
   - Target: 80% coverage
   - **Effort**: 1 week, **Impact**: HIGH

5. **Orchestration Test Suite**
   - 30+ workflow tests
   - Context propagation validation
   - Target: 80% coverage
   - **Effort**: 1 week, **Impact**: HIGH

6. **Enhanced Fallacy Detection**
   - Replace keywords with NLP models
   - Add context awareness
   - Target: F1 ≥0.77
   - **Effort**: 1 week, **Impact**: HIGH

### 5.3 Medium Priority (Week 6-8)

**PRIORITY 2: Improve Existing Tools**

7. **Bias Detection Expansion**
   - Add 20+ more cognitive biases
   - Implement interaction detection
   - **Effort**: 1 week, **Impact**: MEDIUM

8. **Evidence Pipeline Testing**
   - 25+ integration tests
   - Validate propagation correctness
   - **Effort**: 3 days, **Impact**: MEDIUM

9. **Causal-Temporal Integration**
   - Test time-evolved effects
   - Validate timing recommendations
   - **Effort**: 3 days, **Impact**: MEDIUM

### 5.4 Prioritization Formula

```
Priority Score = (Impact × Frequency × Users) / (Effort × Risk)

Impact: 1-10 (how much it improves reasoning)
Frequency: 1-10 (how often tool is used)
Users: 1-10 (how many users affected)
Effort: 1-10 (implementation difficulty)
Risk: 1-10 (chance of breaking things)
```

---

## 6. Success Metrics & Monitoring

### 6.1 Key Performance Indicators

| Metric | Current | Target | Critical |
|--------|---------|--------|----------|
| Logical Reasoning Accuracy | ~10% | 95% | YES |
| Bayesian Update MAE | N/A (wrong) | <0.01 | YES |
| Causal Graph F1 Score | ~40% | >0.8 | YES |
| Fallacy Detection F1 | ~50% | >0.77 | YES |
| Test Coverage | 73.6% | >90% | NO |
| Argument Analysis Coverage | 0% | >80% | YES |
| Orchestration Coverage | 0% | >80% | YES |

### 6.2 Automated Monitoring

**Daily Checks**:
- Run regression test suite
- Check for logic violations
- Validate probability coherence

**Weekly Reports**:
- Accuracy trend analysis
- New failure modes discovered
- Coverage improvements

**Monthly Reviews**:
- Compare against human experts
- Benchmark against baseline systems
- User feedback integration

---

## 7. Implementation Roadmap

### Week 1-2: CRITICAL FIXES
- [ ] Implement real propositional logic parser
- [ ] Fix Bayesian update formula
- [ ] Add basic do-calculus for causal reasoning
- [ ] Create 100 logical reasoning test cases

### Week 3-4: UNTESTED CODE
- [ ] Write 60+ argument analysis tests
- [ ] Write 30+ orchestration tests
- [ ] Write 25+ evidence pipeline tests
- [ ] Achieve 80% coverage on new modules

### Week 5-6: ACCURACY VALIDATION
- [ ] Run FOLIO benchmark (logical reasoning)
- [ ] Run Bayesian inference benchmarks
- [ ] Run CLEVRER (causal reasoning)
- [ ] Run fallacy corpus tests

### Week 7-8: INTEGRATION & POLISH
- [ ] Cross-validation test suite
- [ ] Property-based tests
- [ ] Metamorphic tests
- [ ] Self-testing infrastructure

### Week 9-10: MEASUREMENT & REPORTING
- [ ] Automated accuracy monitoring
- [ ] Comparison with baseline systems
- [ ] Expert evaluation rounds
- [ ] Final accuracy report

---

## 8. Risk Mitigation

### 8.1 Identified Risks

1. **FALSE CONFIDENCE**: Users trust flawed reasoning
   - Mitigation: Add uncertainty quantification, show confidence calibration

2. **CATASTROPHIC ERRORS**: Wrong conclusions in critical decisions
   - Mitigation: Add sanity checks, human-in-the-loop for high stakes

3. **TECHNICAL DEBT**: Quick fixes create more problems
   - Mitigation: Proper architecture, formal verification where possible

### 8.2 Rollback Plan

If accuracy doesn't improve:
1. Revert to simpler, honest "I don't know" responses
2. Document limitations clearly
3. Recommend human expert review for critical reasoning

---

## 9. Conclusion

The unified-thinking MCP server has **significant fundamental flaws** that require immediate attention:

**Critical Issues**:
- ❌ String matching instead of formal logic (~10% accuracy)
- ❌ Mathematically incorrect Bayesian updates
- ❌ No real causal inference theory
- ❌ 0% test coverage on 1,500+ lines of new code
- ❌ Keyword-based fallacy detection (high false positives)

**Path Forward**:
1. Fix the math and logic foundations (Weeks 1-2)
2. Test all untested code (Weeks 3-4)
3. Validate against benchmarks (Weeks 5-6)
4. Achieve measurable accuracy targets (Weeks 7-10)

**Success Definition**:
- Logical reasoning: 95% accuracy
- Bayesian inference: MAE <0.01
- Causal graphs: F1 >0.8
- Fallacy detection: F1 >0.77
- Test coverage: >90%

Only with these fixes will the system provide reliable, trustworthy reasoning that users can depend on.
